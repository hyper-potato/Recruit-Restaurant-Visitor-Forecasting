{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "LSTM (Seq2Seq) -1epoch.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DbyU23AiSCyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, LSTM, GRU, TimeDistributed, Input\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.model_selection import cross_validate,GridSearchCV\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.utils.multiclass import unique_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": false,
        "id": "wztpRFEgSCyY",
        "colab_type": "text"
      },
      "source": [
        "## Load training and testing datasets\n",
        "train: 2016-01-01 : 2017-04-22\n",
        "\n",
        "test: 2017-04-23 : 2017-05-31"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EpWmz0avSCyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_df = pd.read_csv(\"../input/finaldata/sam_train_final.csv\")\n",
        "# test = pd.read_csv(\"../input/finaldata/sam_test_final.csv\")\n",
        "train_df = pd.read_csv(\"../input/finaltable/train_final.csv\")\n",
        "test = pd.read_csv(\"../input/finaltable/test_final.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1Qd0pQzMSCyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = train_df.drop(columns=[ 'population', 'reserve_visitors', 'days_diff', 'day', 'season'])\n",
        "test = test.drop(columns=['population', 'reserve_visitors','days_diff', 'day', 'season'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "CvQVxGpkSCyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# refine column names\n",
        "train_df = train_df.rename({'visitors_x': 'visitors'}, axis = 1)\n",
        "train_df = train_df.rename({'day_of_week_y': 'day_of_week'}, axis = 1)\n",
        "train_df = train_df.rename({'month_y': 'month'}, axis = 1)\n",
        "train_df = train_df.rename({'longitude_y': 'longitude'}, axis = 1)\n",
        "train_df = train_df.rename({'latitude_y': 'latitude'}, axis = 1)\n",
        "test = test.rename({'latitude_y': 'latitude'}, axis = 1)\n",
        "test = test.rename({'longitude_y': 'longitude'}, axis = 1)\n",
        "test = test.rename({'month_y': 'month'}, axis = 1)\n",
        "test = test.rename({'day_of_week_y': 'day_of_week'}, axis = 1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NIFh0sgeSCye",
        "colab_type": "code",
        "outputId": "351658a6-d6b9-4249-d649-09d697372ecb",
        "colab": {}
      },
      "source": [
        "train_df = train_df.loc[:, ~train_df.columns.str.contains('^Unnamed')]\n",
        "train_df.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['visit_date', 'air_store_id', 'Food_Type', 'is_holiday', 'next_day',\n",
              "       'prev_day', 'visitors', 'week', 'daysToPrev25th', 'prev_visitors',\n",
              "       'year', 'month', 'min_visitors', 'mean_visitors', 'median_visitors',\n",
              "       'max_visitors', 'count_observations', 'latitude', 'longitude',\n",
              "       'day_of_week', 'Consecutive_holidays'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "l0qXMlFLSCyh",
        "colab_type": "code",
        "outputId": "04568fae-4e44-47c8-85e9-6cc0acf8e366",
        "colab": {}
      },
      "source": [
        "test = test.loc[:, ~test.columns.str.contains('^Unnamed')]\n",
        "test.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['visit_date', 'air_store_id', 'Food_Type', 'is_holiday', 'next_day',\n",
              "       'prev_day', 'week', 'daysToPrev25th', 'prev_visitors', 'year', 'month',\n",
              "       'min_visitors', 'mean_visitors', 'median_visitors', 'max_visitors',\n",
              "       'count_observations', 'latitude', 'longitude', 'day_of_week',\n",
              "       'Consecutive_holidays'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtNmBhkiSCyj",
        "colab_type": "text"
      },
      "source": [
        "## Encode categorical columns\n",
        "Categorical columns: 'Food_Type','season', 'day_of_week', 'air_store_id'\n",
        "\n",
        "One-hot encoding may provide better result, but I applied labels encoding to avoid high dimensional feature space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YSl69G7RSCyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Weekday\n",
        "le_weekday = LabelEncoder()\n",
        "le_weekday.fit(train_df['day_of_week'])\n",
        "train_df['day_of_week'] = le_weekday.transform(train_df['day_of_week'])\n",
        "test['day_of_week'] = le_weekday.transform(test['day_of_week'])\n",
        "\n",
        "# id\n",
        "le_id = LabelEncoder()\n",
        "le_id.fit(train_df['air_store_id'])\n",
        "train_df['air_store_id'] = le_id.transform(train_df['air_store_id'])\n",
        "test['air_store_id'] = le_id.transform(test['air_store_id'])\n",
        "\n",
        "# food type\n",
        "le_ftype = LabelEncoder()\n",
        "le_ftype.fit(train_df['Food_Type'])\n",
        "train_df['Food_Type'] = le_ftype.transform(train_df['Food_Type'])\n",
        "test['Food_Type'] = le_ftype.transform(test['Food_Type'])\n",
        "\n",
        "# Season\n",
        "# le_season = LabelEncoder()\n",
        "# le_season.fit(train_df['season'])\n",
        "# train_df['season'] = le_season.transform(train_df['season'])\n",
        "# test['season'] = le_season.transform(test['season'])\n",
        "\n",
        "# lbl = LabelEncoder()\n",
        "# # Adjust categorical columns in training set\n",
        "# train_df['Food_Type'] = lbl.fit_transform(train_df['Food_Type'])\n",
        "# train_df['season'] = lbl.fit_transform(train_df['season'])\n",
        "# train_df['day_of_week'] = lbl.fit_transform(train_df['day_of_week'])\n",
        "# train_df['air_store_id'] = lbl.fit_transform(train_df['air_store_id'])\n",
        "# # Adjust categorical columns in test set\n",
        "# test['Food_Type'] = lbl.fit_transform(test['Food_Type'])\n",
        "# test['season'] = lbl.fit_transform(test['season'])\n",
        "# test['day_of_week'] = lbl.fit_transform(test['day_of_week'])\n",
        "# test['air_store_id'] = lbl.fit_transform(test['air_store_id'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Uo-8kSs2SCym",
        "colab_type": "code",
        "outputId": "aa3f63ad-5070-4baa-b345-14aed9faf504",
        "colab": {}
      },
      "source": [
        "train_df['air_store_id'].nunique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "829"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6ZsmZ6fSCyo",
        "colab_type": "text"
      },
      "source": [
        "## Normalization "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVq1pd0sSCyp",
        "colab_type": "text"
      },
      "source": [
        "All features (including one-hot encoded) are normalized to zero mean and unit variance. Each pageviews series normalized independently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMqin3b9SCyq",
        "colab_type": "text"
      },
      "source": [
        "## Fill the cells of missing values\n",
        "\n",
        "replace with 0 or 1 ??？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5t2sWAoKSCyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = train_df.fillna(0)\n",
        "test = test.fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS28_OrwSCys",
        "colab_type": "text"
      },
      "source": [
        "## Simutaneous transformation of Train and test sets\n",
        "\n",
        "Time-independent features (autocorrelations, country, etc) are \"stretched\" to timeseries length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "F5JhygH8SCyt",
        "colab_type": "code",
        "outputId": "d16f116e-2724-4eb4-ce21-a1dc68778eeb",
        "colab": {}
      },
      "source": [
        "# combine train and test sets\n",
        "X_all = train_df.append(test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  sort=sort,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "c7ovdv0lSCyv",
        "colab_type": "code",
        "outputId": "f43aa6ec-a2c6-48a9-ba0b-da091e70c04e",
        "colab": {}
      },
      "source": [
        "X_all.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(284127, 21)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LMCZcFbYSCyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# date table (includes all dates for training and test period)\n",
        "dates = np.arange(np.datetime64(X_all.visit_date.min()),\n",
        "                  np.datetime64(X_all.visit_date.max()) + 1,\n",
        "                  datetime.timedelta(days=1))\n",
        "ids = X_all['air_store_id'].unique()\n",
        "dates_all = dates.tolist()*len(ids)\n",
        "ids_all = np.repeat(ids, len(dates.tolist())).tolist()\n",
        "df_all = pd.DataFrame({\"air_store_id\": ids_all, \"visit_date\": dates_all})\n",
        "df_all['visit_date'] = df_all['visit_date'].copy().apply(lambda x: str(x)[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4tw6hvPsSCyy",
        "colab_type": "code",
        "outputId": "22e1090f-1b61-468f-a22e-90070cfe1462",
        "colab": {}
      },
      "source": [
        "df_all"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        air_store_id  visit_date\n",
              "0                603  2016-01-01\n",
              "1                603  2016-01-02\n",
              "2                603  2016-01-03\n",
              "3                603  2016-01-04\n",
              "4                603  2016-01-05\n",
              "...              ...         ...\n",
              "428588            98  2017-05-27\n",
              "428589            98  2017-05-28\n",
              "428590            98  2017-05-29\n",
              "428591            98  2017-05-30\n",
              "428592            98  2017-05-31\n",
              "\n",
              "[428593 rows x 2 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>air_store_id</th>\n",
              "      <th>visit_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>603</td>\n",
              "      <td>2016-01-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>603</td>\n",
              "      <td>2016-01-02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>603</td>\n",
              "      <td>2016-01-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>603</td>\n",
              "      <td>2016-01-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>603</td>\n",
              "      <td>2016-01-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428588</th>\n",
              "      <td>98</td>\n",
              "      <td>2017-05-27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428589</th>\n",
              "      <td>98</td>\n",
              "      <td>2017-05-28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428590</th>\n",
              "      <td>98</td>\n",
              "      <td>2017-05-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428591</th>\n",
              "      <td>98</td>\n",
              "      <td>2017-05-30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428592</th>\n",
              "      <td>98</td>\n",
              "      <td>2017-05-31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>428593 rows × 2 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkJqVqKESCy0",
        "colab_type": "text"
      },
      "source": [
        "Data relevant to 'visit_date'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Vd5J2PxtSCy0",
        "colab_type": "code",
        "outputId": "0e452a9e-3f97-44f8-b3c6-aab28f337545",
        "colab": {}
      },
      "source": [
        "# create copy of X_all with data relevant to 'visit_date'\n",
        "X_dates = X_all[['visit_date', 'year','month','week', 'is_holiday','next_day','prev_day',\\\n",
        "                 'daysToPrev25th','day_of_week','Consecutive_holidays']].copy()\n",
        "\n",
        "# remove duplicates to avoid memory issues\n",
        "X_dates = X_dates.drop_duplicates('visit_date')\n",
        "\n",
        "# merge dataframe that represents all dates per each restaurant with information about each date\n",
        "df_to_reshape = df_all.merge(X_dates,\n",
        "                             how = \"left\",\n",
        "                             left_on = 'visit_date',\n",
        "                             right_on = 'visit_date')\n",
        "print(df_to_reshape.columns)\n",
        "print(df_to_reshape.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['air_store_id', 'visit_date', 'year', 'month', 'week', 'is_holiday',\n",
            "       'next_day', 'prev_day', 'daysToPrev25th', 'day_of_week',\n",
            "       'Consecutive_holidays'],\n",
            "      dtype='object')\n",
            "(428593, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLj-_XVVSCy5",
        "colab_type": "text"
      },
      "source": [
        "Data relevant to 'air_store_id'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "U9DEqDQKSCy6",
        "colab_type": "code",
        "outputId": "bed8612b-934f-4772-ee82-198f6c1f8835",
        "colab": {}
      },
      "source": [
        "# create copy of X_all with data relevant to 'air_store_id'\n",
        "X_stores = X_all[['air_store_id', 'Food_Type', 'latitude','longitude']].copy()       \n",
        "\n",
        "# remove duplicates to avoid memory issues\n",
        "X_stores = X_stores.drop_duplicates('air_store_id')\n",
        "\n",
        "# merge dataframe that represents all dates per each restaurant with information about each restaurant\n",
        "df_to_reshape = df_to_reshape.merge(X_stores,\n",
        "                                    how = \"left\",\n",
        "                                    left_on = 'air_store_id',\n",
        "                                    right_on = 'air_store_id')\n",
        "print(df_to_reshape.columns)\n",
        "print(df_to_reshape.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['air_store_id', 'visit_date', 'year', 'month', 'week', 'is_holiday',\n",
            "       'next_day', 'prev_day', 'daysToPrev25th', 'day_of_week',\n",
            "       'Consecutive_holidays', 'Food_Type', 'latitude', 'longitude'],\n",
            "      dtype='object')\n",
            "(428593, 14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg8EjaQhSCy8",
        "colab_type": "text"
      },
      "source": [
        "Data relevant to 'air_store_id'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Megfr7NxSCy9",
        "colab_type": "code",
        "outputId": "8235c89a-cdc7-4384-96a5-6be9c13e9da9",
        "colab": {}
      },
      "source": [
        "# merge dataframe that represents all dates per each restaurant with inf. about each restaurant per specific date\n",
        "df_to_reshape = df_to_reshape.merge(X_all[['air_store_id', 'visit_date','prev_visitors', 'mean_visitors', \n",
        "                                       'median_visitors', 'max_visitors', 'min_visitors', 'count_observations','visitors']],\n",
        "                                    how = \"left\",\n",
        "                                    left_on = ['air_store_id', 'visit_date'],\n",
        "                                    right_on = ['air_store_id', 'visit_date'])\n",
        "print(df_to_reshape.columns)\n",
        "print(df_to_reshape.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['air_store_id', 'visit_date', 'year', 'month', 'week', 'is_holiday',\n",
            "       'next_day', 'prev_day', 'daysToPrev25th', 'day_of_week',\n",
            "       'Consecutive_holidays', 'Food_Type', 'latitude', 'longitude',\n",
            "       'prev_visitors', 'mean_visitors', 'median_visitors', 'max_visitors',\n",
            "       'min_visitors', 'count_observations', 'visitors'],\n",
            "      dtype='object')\n",
            "(428593, 21)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_H1lPHE6SCy_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# separate 'visitors' into output array\n",
        "Y_lstm_df = df_to_reshape[['visit_date', 'air_store_id', 'visitors']].copy().fillna(0)\n",
        "\n",
        "# take log(y+1)\n",
        "Y_lstm_df['visitors'] = np.log1p(Y_lstm_df['visitors'].values)\n",
        "\n",
        "# add flag for days when a restaurant was closed\n",
        "df_to_reshape['closed_flag'] = np.where(df_to_reshape['visitors'].isnull() &\n",
        "                                        df_to_reshape['visit_date'].isin(train_df['visit_date']).values,1,0)\n",
        "\n",
        "# drop 'visitors' and from dataset\n",
        "df_to_reshape = df_to_reshape.drop(['visitors'], axis = 1)\n",
        "\n",
        "# fill in NaN values\n",
        "df_to_reshape = df_to_reshape.fillna(-1)\n",
        "\n",
        "# list of df_to_reshape columns without 'air_store_id' and 'visit_date'\n",
        "columns_list = [x for x in list(df_to_reshape.iloc[:,2:])]\n",
        "\n",
        "# bound all numerical values between -1 and 1\n",
        "# note: to avoid data leakage 'fit' should be made on traid data and 'transform' on train and test data\n",
        "# in this case all data in test set is taken from train set, thus fit/transform on all data \n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "scaler.fit(df_to_reshape[columns_list])\n",
        "df_to_reshape[columns_list] = scaler.transform(df_to_reshape[columns_list])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7Um3GKocSCzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SPECIFIC PREPARATION FOR NEURAL NETWORK AND ENCODER/DECODER ---------------\n",
        "# reshape X into (samples, timesteps, features)\n",
        "X_all_lstm = df_to_reshape.values[:,2:].reshape(len(ids),\n",
        "                                                len(dates),\n",
        "                                                df_to_reshape.shape[1]-2)\n",
        "\n",
        "# isolate output for train set and reshape it for time series\n",
        "Y_lstm_df = Y_lstm_df.loc[Y_lstm_df['visit_date'].isin(train_df['visit_date'].values) &\n",
        "                          Y_lstm_df['air_store_id'].isin(train_df['air_store_id'].values),]\n",
        "Y_lstm = Y_lstm_df.values[:,2].reshape(len(train_df['air_store_id'].unique()),\n",
        "                                       len(train_df['visit_date'].unique()),\n",
        "                                       1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "D3ik1TzNSCzE",
        "colab_type": "code",
        "outputId": "e57fd0ce-28e2-4271-d6cf-900a55a6691a",
        "colab": {}
      },
      "source": [
        "# test dates\n",
        "n_test_dates = len(test['visit_date'].unique())\n",
        "\n",
        "# make additional features for number of visitors in t-1, t-2, ... t-7\n",
        "t_minus = np.ones([Y_lstm.shape[0],Y_lstm.shape[1],1])\n",
        "for i in range(1,8):\n",
        "    temp = Y_lstm.copy()\n",
        "    temp[:,i:,:] = Y_lstm[:,0:-i,:].copy()\n",
        "    t_minus = np.concatenate((t_minus[...], temp[...]), axis = 2)\n",
        "t_minus = t_minus[:,:,1:]\n",
        "print (\"t_minus shape\", t_minus.shape)\n",
        "\n",
        "\n",
        "# split X_all into training and test data\n",
        "X_lstm = X_all_lstm[:,:-n_test_dates,:]\n",
        "X_lstm_test = X_all_lstm[:,-n_test_dates:,:]\n",
        "\n",
        "# add t-1, t-2 ... t-7 visitors to feature vector\n",
        "X_lstm = np.concatenate((X_lstm[...], t_minus[...]), axis = 2)\n",
        "\n",
        "# split training set into train and validation sets\n",
        "X_tr = X_lstm[:,39:-140,:]\n",
        "Y_tr = Y_lstm[:,39:-140,:]\n",
        "\n",
        "X_val = X_lstm[:,-140:,:]\n",
        "Y_val = Y_lstm[:,-140:,:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t_minus shape (829, 478, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCV6nmy4SCzH",
        "colab_type": "text"
      },
      "source": [
        "## Specific Preparation for Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAl1tJ1MSCzI",
        "colab_type": "text"
      },
      "source": [
        "This sampling works as effective data augmentation mechanism: training code randomly chooses starting point for each timeseries on each step, generating endless stream of almost non-repeating data.\n",
        "\n",
        "X train dataset: X_lstm\n",
        "\n",
        "Y train dataset: Y_lstm\n",
        "\n",
        "X test dataset: X_lstm_test\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lmc44dqSCzI",
        "colab_type": "text"
      },
      "source": [
        "#### All features for both training and test time periods\n",
        "batch size: 829 unique stores\n",
        "\n",
        "timestep: from 2016-01-01 to 2017-05-31 => 517 days\n",
        "\n",
        "features: 20 features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zWl2DXRLSCzJ",
        "colab_type": "code",
        "outputId": "b69d8e32-e95a-453a-fcac-4095ed96da27",
        "colab": {}
      },
      "source": [
        "X_all_lstm.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(829, 517, 19)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrOfmfniSCzM",
        "colab_type": "text"
      },
      "source": [
        "#### Target variable for training time periods:\n",
        "batch size: 829 unique stores\n",
        "\n",
        "timestep: from 2016-01-01 to 2017-04-22 => 478 days\n",
        "\n",
        "features: 1 target variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "g5pGHGg2SCzM",
        "colab_type": "code",
        "outputId": "09ae2154-369f-4c22-9df1-cdb8f9df0bae",
        "colab": {}
      },
      "source": [
        "Y_lstm.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(829, 478, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9PXhhZ4SCzO",
        "colab_type": "text"
      },
      "source": [
        "#### All features for training time periods:\n",
        "batch size: 829 unique stores\n",
        "\n",
        "timestep: from 2016-01-01 to 2017-04-22 => 478 days\n",
        "\n",
        "features: 20 features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RLdskDZASCzO",
        "colab_type": "code",
        "outputId": "c68b5624-4cba-4d3e-eca3-a8038c02796e",
        "colab": {}
      },
      "source": [
        "X_lstm.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(829, 478, 26)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWh8wNDFSCzR",
        "colab_type": "text"
      },
      "source": [
        "#### All features for test time periods:\n",
        "batch size: 829 unique stores\n",
        "\n",
        "timestep: from 2016-04-23 to 2017-05-31 => 39 days\n",
        "\n",
        "features: 20 features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "b0MT1YtTSCzR",
        "colab_type": "code",
        "outputId": "31ff531c-e67f-4894-a5b4-378b878b275e",
        "colab": {}
      },
      "source": [
        "X_lstm_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(829, 39, 19)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vnC-U6EASCzU",
        "colab_type": "code",
        "outputId": "76ec1dc3-3d3a-435f-e759-361baa900f2f",
        "colab": {}
      },
      "source": [
        "X_tr.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(829, 299, 26)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zrWZamKzSCzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQC7yYeaSCzX",
        "colab_type": "text"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxEKDQH5SCzX",
        "colab_type": "text"
      },
      "source": [
        "-------------------------------------------------- Plan A-----------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oOC0MkLhSCzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_dropout = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "iMpmS9cbSCza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "# First LSTM layer with Dropout regularization\n",
        "model.add(LSTM(200, input_shape=(X_lstm.shape[1],X_lstm.shape[2]), return_sequences=True))\n",
        "# if use_dropout:\n",
        "#     model.add(Dropout(0.2))\n",
        "# # Second LSTM layer\n",
        "# model.add(Dense(100, activation='relu'))\n",
        "# if use_dropout:\n",
        "#     model.add(Dropout(0.2))\n",
        "# The output layer\n",
        "model.add(TimeDistributed(Dense(1))) # apply that Dense function across every output over time\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dzRwciqmSCzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(X_lstm, Y_lstm, epochs=6, batch_size=100, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XuLoGTAsSCzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "V93_KZEySCzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(X_lstm_test, batch_size=478)\n",
        "pred_visitors = scaler.inverse_transform(predictions.reshape(-1, 1))\n",
        "testY_inverse = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "def RMSE(y, pred):\n",
        "    return metrics.mean_squared_error(y, pred)**0.5\n",
        "\n",
        "rmsle = RMSE(testY_inverse, pred_visitors)\n",
        "print('Test RMSLE: %.3f' % rmsle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0kneNFJSCzh",
        "colab_type": "text"
      },
      "source": [
        "-----------------------------------------------------------Plan B-------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_vfex0rSCzh",
        "colab_type": "text"
      },
      "source": [
        "#### Working with long timeseries\n",
        "LSTM/GRU is a great solution for relatively short sequences, up to 100-300 items. On longer sequences LSTM/GRU still works, but can gradually forget information from the oldest items.  Competition timeseries is up to 478 days long, so I have to find some method to \"strengthen\" GRU memory. \n",
        "\n",
        "The encoder takes input features of 39 days (t1, t2 … t39) and encode their hidden states through LSTM neural network. Then it pass the hidden states to decoder. Decoder use them with the features of 39 days shifted 1 day forward (t2, t3 … T40) to predict number of visitors per each of 829 restaurants in t_40.\n",
        "\n",
        "\n",
        "\n",
        "#### Training and validation\n",
        "There are two ways to split timeseries into training and validation datasets:\n",
        "\n",
        "1. Walk-forward split. This is not actually a split: we train on full dataset and validate on full dataset, using different timeframes. Timeframe for validation is shifted forward by one prediction interval relative to timeframe for training.\n",
        "\n",
        "2. Side-by-side split. This is traditional split model for mainstream machine learning. Dataset splits into independent parts, one part used strictly for training and another part used strictly for validation.\n",
        "\n",
        "Walk-forward is preferable, because it directly relates to the competition goal: predict future values using historical values. But this split consumes datapoints at the end of timeseries, thus making hard to train model to precisely predict the future. \n",
        "\n",
        "I used validation (with walk-forward split) only for model tuning. Final model to predict future values was trained in blind mode, without any validation.\n",
        "\n",
        "#### Losses and regularization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1BEpATH7SCzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ENCODER-DECODER MODEL ===================================================\n",
        "# many thanks to the following resources:\n",
        "# https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
        "# https://blog.keras.io/building-autoencoders-in-keras.html\n",
        "# http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf\n",
        "# https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/\n",
        "# https://github.com/Arturus/kaggle-web-traffic\n",
        "\n",
        "# MODEL FOR ENCODER AND DECODER -------------------------------------------\n",
        "num_encoder_tokens = X_lstm.shape[2]\n",
        "latent_dim = 64 # to avoid \"kernel run out of time\" situation. I used 256.\n",
        "\n",
        "# encoder training\n",
        "encoder_inputs = Input(shape = (None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, \n",
        "               batch_input_shape = (1, None, num_encoder_tokens),\n",
        "               stateful = False,\n",
        "               return_sequences = True,\n",
        "               return_state = True,\n",
        "               recurrent_initializer = 'glorot_uniform')\n",
        "\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c] # 'encoder_outputs' are ignored and only states are kept.\n",
        "\n",
        "# Decoder training, using 'encoder_states' as initial state.\n",
        "decoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "\n",
        "decoder_lstm_1 = GRU(latent_dim,\n",
        "                      batch_input_shape = (1, None, num_encoder_tokens),\n",
        "                      stateful = False,\n",
        "                      return_sequences = True,\n",
        "                      return_state = False,\n",
        "                      dropout = 0.2,\n",
        "                      recurrent_dropout = 0.2) # True\n",
        "\n",
        "decoder_lstm_2 = GRU(32, # to avoid \"kernel run out of time\" situation. I used 128.\n",
        "                     stateful = False,\n",
        "                     return_sequences = True,\n",
        "                     return_state = True,\n",
        "                     dropout = 0.2,\n",
        "                     recurrent_dropout = 0.2)\n",
        "\n",
        "decoder_outputs, _, _ = decoder_lstm_2(decoder_lstm_1(decoder_inputs, initial_state = encoder_states))\n",
        "decoder_dense = TimeDistributed(Dense(Y_lstm.shape[2], activation = 'relu'))\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# training model\n",
        "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "training_model.compile(optimizer = 'adam', loss = 'mean_squared_error')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aHKpTylOSCzl",
        "colab_type": "code",
        "outputId": "5237f74d-02b5-4b49-bed5-fb2f833407cb",
        "colab": {}
      },
      "source": [
        "# useful for understanding the model architecture\n",
        "training_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            (None, None, 26)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            (None, None, 26)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   [(None, None, 64), ( 23296       input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   (None, None, 64)     23296       input_6[0][0]                    \n",
            "                                                                 lstm_4[0][1]                     \n",
            "                                                                 lstm_4[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_6 (LSTM)                   [(None, None, 32), ( 12416       lstm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, None, 1)      33          lstm_6[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 59,041\n",
            "Trainable params: 59,041\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EKcTrO_nSCzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GENERATOR APPLIED TO FEED ENCODER AND DECODER ---------------------------\n",
        "# generator that randomly creates times series of 39 consecutive days\n",
        "# theses time series has following 3d shape: 829 restaurants * 39 days * num_features \n",
        "def dec_enc_n_days_gen(X_3d, Y_3d, length):\n",
        "    while 1:\n",
        "        decoder_boundary = X_3d.shape[1] - length - 1\n",
        "        \n",
        "        encoder_start = np.random.randint(0, decoder_boundary)\n",
        "        encoder_end = encoder_start + length\n",
        "        \n",
        "        decoder_start = encoder_start + 1\n",
        "        decoder_end = encoder_end + 1\n",
        "        \n",
        "        X_to_conc = X_3d[:, encoder_start:encoder_end, :]\n",
        "        Y_to_conc = Y_3d[:, encoder_start:encoder_end, :]\n",
        "        X_to_decode = X_3d[:, decoder_start:decoder_end, :]\n",
        "        Y_decoder = Y_3d[:, decoder_start:decoder_end, :]\n",
        "        \n",
        "        yield([X_to_conc,\n",
        "               X_to_decode],\n",
        "               Y_decoder)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9O8R1NB5SCzq",
        "colab_type": "code",
        "outputId": "81a2fb9e-c322-4eb7-f994-12768fbae489",
        "colab": {}
      },
      "source": [
        "# TRAINING -------------------------------------------------------------\n",
        "# Training on X_tr/Y_tr and validate with X_val/Y_val\n",
        "# To perform validation training on validation data should be\n",
        "# made instead of training on full data set.\n",
        "# Then validation check is made on period outside of training data\n",
        "# (included in code below).\n",
        "'''\n",
        "training_model.fit_generator(dec_enc_n_days_gen(X_tr, Y_tr, 39),\n",
        "                             validation_data = dec_enc_n_days_gen(X_val, Y_val, 39),\n",
        "                             steps_per_epoch = X_lstm.shape[0],\n",
        "                             validation_steps = X_val.shape[0],\n",
        "                             verbose = 1,\n",
        "                             epochs = 1)\n",
        "'''\n",
        "\n",
        "# Training on full dataset\n",
        "training_model.fit_generator(dec_enc_n_days_gen(X_lstm[:,:,:], Y_lstm[:,:,:], 39),\n",
        "                            steps_per_epoch = X_lstm[:,:,:].shape[0],\n",
        "                            verbose = 1,\n",
        "                            epochs = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "829/829 [==============================] - 179s 216ms/step - loss: 0.4050\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f92a067cba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "x0kjzFW1SCzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PREDICTION FUNCTION --------------------------------------------------\n",
        "\n",
        "# function takes 39 days before first prediction day (input_seq)\n",
        "# then using encoder to identify hidden states for these 39 days.\n",
        "# Next, decoder takes hidden states provided by encoder\n",
        "# and predicts number of visitors from day 2 to day 40.\n",
        "# Day 40 is the first day of target_seq.\n",
        "\n",
        "# Predicted value for day 40 is appended to features of day 41.\n",
        "# Then function takes period from day 2 to day 40 and repeat the process\n",
        "# unil all days in target sequence get their predictions. \n",
        "\n",
        "# The output of the function is the vector with predictions that has\n",
        "# following shape: 820 restaurants * 39 days * 1 predicted visitors amount\n",
        "\n",
        "def predict_sequence(inf_enc, inf_dec, input_seq, Y_input_seq, target_seq):\n",
        "    # state of input sequence produced by encoder\n",
        "    state = inf_enc.predict(input_seq)\n",
        "    \n",
        "    # restrict target sequence to the same shape as X_lstm_test\n",
        "    target_seq = target_seq[:,:, :X_lstm_test.shape[2]]\n",
        "    \n",
        "    \n",
        "    # create vector that contains y for previous 7 days\n",
        "    t_minus_seq = np.concatenate((Y_input_seq[:,-1:,:], input_seq[:,-1:, X_lstm_test.shape[2]:-1]), axis = 2)\n",
        "    \n",
        "    # current sequence that is going to be modified each iteration of the prediction loop\n",
        "    current_seq = input_seq.copy()\n",
        "    \n",
        "    \n",
        "    # predicting outputs\n",
        "    output = np.ones([target_seq.shape[0],1,1])\n",
        "    for i in range(target_seq.shape[1]):\n",
        "        # add visitors for previous 7 days into features of a new day\n",
        "        new_day_features = np.concatenate((target_seq[:,i:i+1,:], t_minus_seq[...]), axis = 2)\n",
        "        \n",
        "        # move prediction window one day forward\n",
        "        current_seq = np.concatenate((current_seq[:,1:,:], new_day_features[:,]), axis = 1)\n",
        "        \n",
        "        \n",
        "        # predict visitors amount\n",
        "        pred = inf_dec.predict([current_seq] + state)\n",
        "        \n",
        "        # update t_minus_seq\n",
        "        t_minus_seq = np.concatenate((pred[:,-1:,:], t_minus_seq[...]), axis = 2)\n",
        "        t_minus_seq = t_minus_seq[:,:,:-1]        \n",
        "        \n",
        "        # update predicitons list\n",
        "        output = np.concatenate((output[...], pred[:,-1:,:]), axis = 1)\n",
        "        \n",
        "        # update state\n",
        "        state = inf_enc.predict(current_seq)\n",
        "    \n",
        "    return output[:,1:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PI6UH7RVSCzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# INFERENCE ENCODER AND DECODER -----------------------------------------    \n",
        "# inference encoder\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# inference decoder\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs,_,_ = decoder_lstm_2(decoder_lstm_1(decoder_inputs,\n",
        "                                                    initial_state = decoder_states_inputs))\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
        "                      [decoder_outputs])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tbg0SD2TSCzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicting test values\n",
        "enc_dec_pred = predict_sequence(encoder_model,\n",
        "                                decoder_model,\n",
        "                                X_lstm[:,-X_lstm_test.shape[1]:,:],\n",
        "                                Y_lstm[:,-X_lstm_test.shape[1]:,:],\n",
        "                                X_lstm_test[:,:,:])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Fy9SK_62SCzv",
        "colab_type": "code",
        "outputId": "b1c6d58e-5ddd-4aba-fd1d-a109c811893b",
        "colab": {}
      },
      "source": [
        "# test data\n",
        "sample_sub = pd.read_csv('../input/recruit-restaurant-visitor-forecasting/sample_submission.csv')\n",
        "# transform test data\n",
        "air_test = sample_sub.copy()\n",
        "air_test['air_store_id'] = air_test['id'].apply(lambda x: str(x)[:-11])\n",
        "air_test['visit_date'] = air_test['id'].apply(lambda x: str(x)[-10:])\n",
        "\n",
        "# dataframe for predictions\n",
        "submission_lstm = air_test.copy()\n",
        "submission_lstm.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                id  visitors          air_store_id  visit_date\n",
              "0  air_00a91d42b08b08d9_2017-04-23         0  air_00a91d42b08b08d9  2017-04-23\n",
              "1  air_00a91d42b08b08d9_2017-04-24         0  air_00a91d42b08b08d9  2017-04-24\n",
              "2  air_00a91d42b08b08d9_2017-04-25         0  air_00a91d42b08b08d9  2017-04-25\n",
              "3  air_00a91d42b08b08d9_2017-04-26         0  air_00a91d42b08b08d9  2017-04-26\n",
              "4  air_00a91d42b08b08d9_2017-04-27         0  air_00a91d42b08b08d9  2017-04-27"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>visitors</th>\n",
              "      <th>air_store_id</th>\n",
              "      <th>visit_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-04-23</td>\n",
              "      <td>0</td>\n",
              "      <td>air_00a91d42b08b08d9</td>\n",
              "      <td>2017-04-23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-04-24</td>\n",
              "      <td>0</td>\n",
              "      <td>air_00a91d42b08b08d9</td>\n",
              "      <td>2017-04-24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-04-25</td>\n",
              "      <td>0</td>\n",
              "      <td>air_00a91d42b08b08d9</td>\n",
              "      <td>2017-04-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-04-26</td>\n",
              "      <td>0</td>\n",
              "      <td>air_00a91d42b08b08d9</td>\n",
              "      <td>2017-04-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>air_00a91d42b08b08d9_2017-04-27</td>\n",
              "      <td>0</td>\n",
              "      <td>air_00a91d42b08b08d9</td>\n",
              "      <td>2017-04-27</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6w8Gg6mXSCzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add predicted test values to submission dataset ---------------------\n",
        "\n",
        "# Note: it is important to preserve the order of time series.\n",
        "# Thus, test set will contain all 829 lines in the same order as train set.\n",
        "# To make this 'air_store_id' is taken as in X and not in X_test (second line of 'test' variable below).\n",
        "# Only relevant results will be merged for submission dataframe\n",
        "test_df = df_to_reshape.loc[df_to_reshape['visit_date'].isin(test['visit_date'].values) &\n",
        "                         df_to_reshape['air_store_id'].isin(train_df['air_store_id'].values),]\n",
        "\n",
        "\n",
        "# reshape predicted values to initial shape\n",
        "test_pred = enc_dec_pred.reshape(test_df.shape[0], 1)\n",
        "test_pred_exp = np.exp(test_pred) - 1.0\n",
        "test_pred_exp[test_pred_exp<0] = 0\n",
        "\n",
        "# add predictions to dataframe with 'air_store_id' and 'visit_date'\n",
        "test_df_pred = test_df[['air_store_id', 'visit_date']].copy()\n",
        "test_df_pred['predicted'] = test_pred_exp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "v-mEl-iTSCzy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reverse transform of 'air_store_id'\n",
        "test_df_pred['air_store_id'] = le_id.inverse_transform(test_df_pred['air_store_id'])\n",
        "\n",
        "# finalizing submission csv file\n",
        "submission_df = submission_lstm.merge(test_df_pred,\n",
        "                                     how = 'left',\n",
        "                                     left_on = ['air_store_id', 'visit_date'],\n",
        "                                     right_on = ['air_store_id', 'visit_date'])\n",
        "\n",
        "submission_df['visitors'] = submission_df['predicted']\n",
        "submission_df = submission_df.drop(['air_store_id', 'visit_date', 'predicted'], axis = 1)\n",
        "submission_df.to_csv('submission.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}